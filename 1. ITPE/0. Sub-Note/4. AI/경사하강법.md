#AI #딥러닝 #최적화 #필수

## 정의
손실 함수의 최소값을 찾는 최적화 알고리즘, 경사하강법
- 함수의 최소값 위치를 찾기 위해 경사가 하강하는 방향으로 조금씩 이동하면서 근사값을 찾는 알고리즘
- 미분의 개념을 최적화 문제에 적용, 에러 함수의 Global Minimum을 찾는 방법
## 키워드
* SGD, Momentum, Adam, 학습률(Learning Rate), Local Minimum
## 암기법
* 유형: ==확배미== (확률적, 배치, 미니배치)
* 해결: ==SMNARA== (SGD, Momentum, NAG, AdaGrad, RMSProp, Adam)
## 연관 토픽
- [[활성화 함수(Activation Function)]] - 비선형 변환
- [[과적합(Overfitting) 문제]] - 일반화 문제
- [[최적화 알고리즘 (Optimization Algorithm)]] - 최적화 기법
## 유형 (==확배미==)
| 유형                 | 설명                     | 특징                |
| ------------------ | ---------------------- | ----------------- |
| ==확==률적 경사하강법(SGD) | 하나의 학습 데이터마다 즉시 오차 계산  | 순차적 수행, 병렬컴퓨팅 어려움 |
| ==배==치 경사하강법       | 모든 데이터 학습 후 가중치 한번에 갱신 | 성능 문제, 메모리 부담     |
| ==미==니배치 경사하강법     | Sampling 선택 학습 후 갱신    | GPU 기반 효율적 병렬컴퓨팅  |
## 문제점 및 해결
| 문제점 | 설명 | 해결 기법 |
|--------|------|-----------|
| Over Shooting | 최적점을 지나침 | 학습률 조정 |
| Local Minimum | 지역 최소값에 갇힘 | Momentum, Adam |
| Saddle Point | 안장점에서 정체 | NAG, RMSProp |
## Local Minimum 해결 기법 (==SMNARA==)
- ==S==GD: 확률적 경사하강법
- ==M==omentum: 이전 시간의 gradient까지 고려
- ==N==AG(Nesterov Accelerated Gradient): 이동 중지 지점 지나치는 문제 해결
- ==A==daGrad: 학습률 자동 조정
- ==R==MSProp: AdaGrad 개선
- ==A==dam: Momentum + RMSProp 결합 (가장 대중적)
