#AI #보안 #머신러닝 #필수

## 정의
ML 모델을 속이기 위한 악의적 입력 공격, 적대적 공격
- 머신러닝 모델의 취약점을 이용하여 의도적으로 잘못된 예측을 유도하는 공격 기법
- 인간이 인식하기 어려운 미세한 변형으로 모델을 속임
## 키워드
* Adversarial Attack, FGSM, Poisoning, Evasion, Defense GAN
## 암기법
* 적대적 공격: ==포이즈닝(학습) + 이베이전(추론)==
* 방어: ==적대적 훈련, Defense GAN==
## 연관 토픽
- [[GAN (Generative Adversarial Network)]] - Defense GAN
- [[딥페이크]] - 악용 사례
- [[OWASP LLM Top 10]] - LLM 보안
## 공격 유형
```
┌─────────────────────────────────────────────────┐
│              적대적 공격 유형                    │
│                                                  │
│  [학습 단계]              [추론 단계]           │
│      │                        │                 │
│      ▼                        ▼                 │
│  ┌──────────┐           ┌──────────┐           │
│  │ 데이터   │           │ 회피     │           │
│  │ 오염     │           │ 공격     │           │
│  │Poisoning │           │ Evasion  │           │
│  └──────────┘           └──────────┘           │
│      │                        │                 │
│      ▼                        ▼                 │
│  백도어 삽입              적대적 예제           │
└─────────────────────────────────────────────────┘
```
## 공격 분류
| 분류 기준 | 유형 | 설명 |
|-----------|------|------|
| 공격 시점 | ==Poisoning== | 학습 데이터 오염 |
| | ==Evasion== | 추론 시 회피 공격 |
| 지식 수준 | ==White-box== | 모델 구조 알고 있음 |
| | ==Black-box== | 모델 구조 모름 |
| 목표 | ==Targeted== | 특정 클래스로 오분류 |
| | ==Untargeted== | 아무 오분류 |
## 주요 공격 기법
| 기법 | 설명 |
|------|------|
| ==FGSM== | Fast Gradient Sign Method |
| ==PGD== | Projected Gradient Descent |
| ==C&W== | Carlini & Wagner Attack |
| ==DeepFool== | 최소 섭동 탐색 |
| ==Patch Attack== | 물리적 패치 부착 |
## 적대적 예제 생성
```
x_adv = x + ε × sign(∇_x L(θ, x, y))

x: 원본 입력
ε: 섭동 크기
∇_x L: 손실 함수의 입력에 대한 기울기
x_adv: 적대적 예제
```
## 방어 기법
| 기법 | 설명 |
|------|------|
| ==Adversarial Training== | 적대적 예제로 학습 |
| ==Defense GAN== | GAN 기반 입력 정화 |
| ==Input Transformation== | 입력 전처리 |
| ==Certified Defense== | 수학적 보장 방어 |
| ==Ensemble== | 앙상블 모델 |
## 실제 위협 사례
- 자율주행: 표지판 오인식 유도
- 얼굴인식: 인증 우회
- 악성코드 탐지: 탐지 회피
- 음성인식: 숨겨진 명령 주입
