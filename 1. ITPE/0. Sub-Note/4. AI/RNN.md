#AI #딥러닝 #자연어처리 #필수

## 정의
순차적 데이터 처리를 위한 순환 신경망, RNN
- 신경망 연결에 순환구조를 넣어 시계열 데이터를 처리하는 딥러닝 모델
- 음성이나 언어 등 연속된 입력 데이터에 적용이 용이한 신경망
## 키워드
* Recurrent, Hidden State, 시계열, 장기 의존성, 기울기 소실
## 암기법
* RNN = Recurrent(순환) + Neural Network
## 연관 토픽
- [[LSTM]] - 장단기 메모리
- [[CNN]] - 합성곱 신경망
- [[트랜스포머(Transformer)]] - 어텐션 기반 모델
## 특징
- ==순환 구조==: 이전 시점의 출력이 현재 시점의 입력으로 사용
- ==시계열 처리==: 연속된 데이터의 순서 정보 학습
- ==가변 길이 입력==: 다양한 길이의 시퀀스 처리 가능
- ==은닉 상태(Hidden State)==: 이전 정보를 저장하고 전달
## 구조
```
             ┌─────────────────────────┐
             │                         │
             ▼                         │
[x_t] → [RNN Cell] → [h_t] → [y_t]     │
              │                        │
              └────────────────────────┘
              (순환 연결)
```
## 문제점
- ==장기 의존성 문제(Long-term Dependency)==: 긴 시퀀스에서 초기 정보 손실
- ==기울기 소실(Vanishing Gradient)==: 역전파 시 기울기가 0에 수렴
- ==기울기 폭발(Exploding Gradient)==: 기울기가 무한대로 발산
## 해결 방안
- [[LSTM]]: 게이트 메커니즘으로 장기 의존성 해결
- [[GRU(Gated Recurrent Unit)]]: LSTM 간소화 버전
- 기울기 클리핑(Gradient Clipping): 기울기 폭발 방지
## 활용 분야
- 자연어 처리 (NLP): 기계 번역, 텍스트 생성
- 음성 인식 (Speech Recognition)
- 시계열 예측: 주가 예측, 날씨 예측
- 감성 분석 (Sentiment Analysis)
