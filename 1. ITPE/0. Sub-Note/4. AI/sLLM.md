#AI #LLM #경량화

## 정의
경량화된 대규모 언어 모델, sLLM
- 매개변수를 줄이고 파인튜닝으로 최적화한 경량화 언어모델
- 온디바이스 AI, 엣지 컴퓨팅 환경에서 실행 가능한 소형 LLM
## 키워드
* sLLM, 경량화, 양자화, 지식 증류, 온디바이스, LoRA
## 암기법
* sLLM = smaller(작은) + LLM
* 경량화: ==양지가로== (양자화, 지식증류, 가지치기, LoRA)
## 연관 토픽
- [[LLM]] - 대규모 언어 모델
- [[온디바이스 AI(Artificial Intelligence)]] - 엣지 AI
- [[지식 증류(Knowledge Distillation)]] - 모델 압축
## LLM vs sLLM
| 구분 | LLM | sLLM |
|------|-----|------|
| 파라미터 | 수십억~수천억 | 수억~수십억 |
| 실행 환경 | 클라우드/서버 | 엣지/온디바이스 |
| 비용 | 높음 | 낮음 |
| 지연시간 | 높음 | 낮음 |
| 범용성 | 높음 | 도메인 특화 |
## 경량화 기법
| 기법 | 설명 |
|------|------|
| ==양자화(Quantization)== | 가중치 비트 수 감소 (FP32→INT8) |
| ==지식 증류(Knowledge Distillation)== | 대형 모델 지식을 소형 모델로 전이 |
| ==가지치기(Pruning)== | 불필요한 가중치 제거 |
| ==LoRA== | 저랭크 적응 (Low-Rank Adaptation) |
## 주요 sLLM 모델
| 모델 | 개발사 | 파라미터 |
|------|--------|----------|
| Phi-3 | Microsoft | 3.8B |
| Gemma | Google | 2B, 7B |
| LLaMA 3 | Meta | 8B |
| Mistral | Mistral AI | 7B |
| Qwen | Alibaba | 1.8B~72B |
## 활용 분야
- ==온디바이스 AI==: 스마트폰, IoT 기기
- ==엣지 컴퓨팅==: 실시간 처리 필요 환경
- ==프라이버시==: 데이터 로컬 처리
- ==비용 절감==: 클라우드 API 비용 절감
## 장단점
| 장점 | 단점 |
|------|------|
| 낮은 비용 | 성능 제한 |
| 빠른 응답 | 범용성 부족 |
| 프라이버시 보호 | 복잡한 작업 어려움 |
| 오프라인 가능 | 지속적 업데이트 어려움 |
