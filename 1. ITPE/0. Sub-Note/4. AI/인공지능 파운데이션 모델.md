#AI #LLM #사전학습

## 정의
대규모 데이터로 사전 학습된 범용 AI 모델, 파운데이션 모델
- 대규모 데이터셋으로 사전 학습되어 다양한 다운스트림 작업에 적용 가능한 기반 모델
- 하나의 모델로 다양한 작업(텍스트, 이미지, 코드 등)을 수행할 수 있는 범용 모델
## 키워드
* Foundation Model, 사전학습, 파인튜닝, 전이학습, 범용 AI
## 암기법
* Foundation = 기반/토대 → 다양한 응용의 기반이 되는 모델
* 사전학습 → 파인튜닝 → 배포
## 연관 토픽
- [[LLM]] - 대규모 언어 모델
- [[파인튜닝(Fine-tuning)]] - 미세 조정
- [[트랜스포머(Transformer)]] - 기반 아키텍처
## 특징
| 특징 | 설명 |
|------|------|
| ==대규모 학습== | 수십억~수조 토큰 학습 |
| ==범용성== | 다양한 작업에 적용 가능 |
| ==전이 학습== | 파인튜닝으로 특화 가능 |
| ==창발적 능력== | 학습하지 않은 능력 발현 |
## 파운데이션 모델 유형
```
┌─────────────────────────────────────────────────┐
│           파운데이션 모델 유형                       │
│                                                 │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐       │
│  │  언어     │  │   비전    │  │ 멀티모달   │        │
│  │  모델     │  │   모델    │  │  모델     │       │
│  │          │  │          │  │          │       │
│  │ GPT,BERT │  │ ViT,CLIP │  │ GPT-4V   │       │ 
│  │ LLaMA    │  │ DINO     │  │ Gemini   │       │ 
│  └──────────┘  └──────────┘  └──────────┘       │
└─────────────────────────────────────────────────┘
```
## 주요 파운데이션 모델
| 모델 | 개발사 | 유형 |
|------|--------|------|
| GPT-4 | OpenAI | 언어/멀티모달 |
| Claude | Anthropic | 언어 |
| Gemini | Google | 멀티모달 |
| LLaMA | Meta | 언어 (오픈소스) |
| BERT | Google | 언어 (이해) |
| CLIP | OpenAI | 비전-언어 |
## 학습 패러다임
1. ==사전 학습(Pre-training)==: 대규모 비라벨 데이터로 학습
2. ==파인튜닝(Fine-tuning)==: 특정 작업에 맞게 조정
3. ==프롬프트 엔지니어링==: 프롬프트로 작업 지시
4. ==In-Context Learning==: 예시 기반 학습
## 활용 방식
| 방식 | 설명 |
|------|------|
| Zero-shot | 예시 없이 작업 수행 |
| Few-shot | 소수 예시로 학습 |
| Fine-tuning | 추가 학습으로 특화 |
| RAG | 외부 지식 결합 |
