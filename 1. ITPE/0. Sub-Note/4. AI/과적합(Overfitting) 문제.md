#AI #딥러닝 #모델최적화 #필수

## 정의
학습 데이터에 과도하게 최적화되어 일반화 성능이 저하되는 문제, 과적합
- 감독학습(Supervised Learning)에서 과거의 학습데이터에 대해서는 잘 예측하지만 새로 들어온 데이터에 대해서 성능이 떨어져서 일반화가 어려운 문제
- 모델의 학습 오차가 매우 낮은데 비해 테스트 오차가 높은 현상
## 키워드
* Overfitting, Underfitting, Dropout, Regularization, Cross Validation
## 암기법
* 과적합 해결: ==드조정배== (드롭아웃, 조기종료, 정규화, 배치정규화)
## 연관 토픽
- [[경사하강법]] - 최적화 알고리즘
- [[활성화 함수(Activation Function)]] - 비선형 변환
- [[혼동 행렬]] - 모델 평가
## 과적합 vs 과소적합
| 구분 | 과적합(Overfitting) | 과소적합(Underfitting) |
|------|---------------------|----------------------|
| 원인 | 모델 복잡도 높음 | 모델 복잡도 낮음 |
| 학습 오차 | 낮음 | 높음 |
| 테스트 오차 | 높음 | 높음 |
| 편향(Bias) | 낮음 | 높음 |
| 분산(Variance) | 높음 | 낮음 |
## 해결 기법
### 데이터 관점
- ==데이터 증강(Data Augmentation)==: 학습 데이터 확장
- ==교차 검증(Cross Validation)==: K-Fold 검증으로 일반화 성능 평가
### 모델 관점
- ==드롭아웃(Dropout)==: 무작위로 은닉 노드 비활성화
- ==조기 종료(Early Stopping)==: 검증 오차 증가 시 학습 중단
- ==정규화(Regularization)==: L1(Lasso), L2(Ridge) 규제 적용
- ==배치 정규화(Batch Normalization)==: 입력 분포 정규화
### 앙상블 관점
- ==배깅(Bagging)==: Bootstrap Aggregating
- ==부스팅(Boosting)==: 순차적 학습
- ==랜덤 포레스트==: 다수 결정 트리 앙상블
## 드롭아웃(Dropout)
- 인공지능 역전파 수행시 과적합 문제를 해결하기 위해 무작위로 은닉 노드를 비활성화하는 기법
- 특징: 과적합 문제 해결, 모델 간소화
- 단점: 무작위 노드 제거/복원으로 인한 오버헤드, 학습속도 지연
