#AI #LLM #딥러닝

## 정의
수천억 이상의 파라미터를 가진 대규모 AI 모델, 초대규모 AI 모델
- 대규모 데이터와 컴퓨팅 자원을 활용하여 학습된 수천억~수조 개의 파라미터를 가진 인공지능 모델
- 창발적 능력(Emergent Abilities)을 보이는 대형 모델
## 키워드
* 초대규모, 창발적 능력, 파라미터, Few-shot, In-context
## 암기법
* 초대규모 = 수천억 이상 파라미터
* 특징: ==창범퓨인== (창발적 능력, 범용성, Few-shot, In-context)
## 연관 토픽
- [[LLM]] - 대규모 언어 모델
- [[인공지능 파운데이션 모델]] - 기반 모델
- [[sLLM]] - 경량화 모델
## 규모 변화
```
┌─────────────────────────────────────────────────┐
│              AI 모델 규모 변화                     │
│                                                 │
│  BERT (2018)     → 340M 파라미터                  │
│  GPT-2 (2019)    → 1.5B 파라미터                  │
│  GPT-3 (2020)    → 175B 파라미터                  │
│  GPT-4 (2023)    → 1.76T 파라미터 (추정)           │
│  Gemini (2024)   → 멀티모달 초대규모                │
└─────────────────────────────────────────────────┘
```
## 특징
| 특징 | 설명 |
|------|------|
| ==창발적 능력== | 학습하지 않은 새로운 능력 발현 |
| ==범용성== | 다양한 작업에 적용 가능 |
| ==Few-shot 학습== | 소수 예시로 작업 수행 |
| ==In-context 학습== | 프롬프트 내 예시로 학습 |
## 창발적 능력 (Emergent Abilities)
- 특정 규모 이상에서 갑자기 나타나는 능력
- 산술 추론, 다단계 추론
- 코드 생성, 번역
- 상식 추론
## 학습 요구사항
| 요소 | 요구 수준 |
|------|-----------|
| ==데이터== | 수조 토큰 |
| ==컴퓨팅== | 수천~수만 GPU |
| ==비용== | 수백만~수천만 달러 |
| ==시간== | 수주~수개월 |
## 과제 및 해결
| 과제 | 해결 방안 |
|------|-----------|
| 학습 비용 | 효율적 학습, 모델 압축 |
| 추론 비용 | 양자화, 지식 증류, sLLM |
| 환각 | RAG, 팩트체크 |
| 편향 | 데이터 정제, RLHF |
## 주요 초대규모 모델
| 모델 | 개발사 | 파라미터 |
|------|--------|----------|
| GPT-4 | OpenAI | 1.76T (추정) |
| Gemini | Google | 비공개 |
| Claude | Anthropic | 비공개 |
| LLaMA 3 | Meta | 405B |
