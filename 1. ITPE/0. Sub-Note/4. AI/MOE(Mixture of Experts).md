#AI #LLM #아키텍처

## 정의
전문가 네트워크를 선택적으로 활성화하는 아키텍처, MOE
- 여러 전문가(Expert) 네트워크 중 입력에 따라 일부만 선택적으로 활성화하여 효율적으로 처리하는 딥러닝 아키텍처
- 모델 크기 대비 계산 비용을 크게 줄이는 희소(Sparse) 모델
## 키워드
* Mixture of Experts, 게이팅, Router, Sparse Model, Top-K
## 암기법
* MOE = 전문가 팀에서 적합한 전문가만 선택
* Mixture(혼합) + Experts(전문가)
## 연관 토픽
- [[LLM]] - 대규모 언어 모델
- [[트랜스포머(Transformer)]] - 기반 아키텍처
- [[COT(Chain of Thought)]] - 추론 기법
- [[RIG(Retrieval Interleaved Generation)]]
* [[RAG(Retrieval Augmented Generation)]]
* [[MCP(Model Context Protocol)]]
## 구조
```
┌─────────────────────────────────────────────────┐
│                    입력 (x)                      │
│                       │                         │
│                       ▼                         │
│              ┌─────────────┐                    │
│              │     게이팅    │                    │
│              │    네트워크   │                    │
│              │   (Router)  │                    │
│              └──────┬──────┘                    │
│         ┌───────┬───┴───┬───────┐               │
│         ▼       ▼       ▼       ▼               │
│     ┌──────┐┌──────┐┌──────┐┌──────┐            │
│     │Expert││Expert││Expert││Expert│            │
│     │  1   ││  2   ││  3   ││  N   │            │
│     └──────┘└──────┘└──────┘└──────┘            │
│         │       │       │       │               │
│         └───────┴───┬───┴───────┘               │
│                     ▼                           │
│                  출력 (y)                        │
└─────────────────────────────────────────────────┘
```
## 핵심 구성요소
- ==전문가(Expert)==: 특정 패턴/도메인에 특화된 서브네트워크
- ==게이팅 네트워크(Router)==: 입력에 따라 활성화할 전문가 선택
- ==Top-K 선택==: 상위 K개 전문가만 활성화 (보통 K=2)
- ==로드 밸런싱==: 전문가 간 부하 균형 유지
## 장점
| 장점 | 설명 |
|------|------|
| 효율성 | 일부 전문가만 활성화 → 계산 비용 감소 |
| 확장성 | 전문가 추가로 모델 확장 용이 |
| 전문화 | 각 전문가가 특정 패턴 학습 |
| 성능 | 동일 계산량 대비 높은 성능 |
## 주요 MOE 모델
| 모델 | 특징 |
|------|------|
| Mixtral 8x7B | 8개 전문가, 7B 파라미터 |
| DeepSeek | MOE + MLA 결합 |
| GPT-4 | MOE 기반 추정 |
| Switch Transformer | Google, 단일 전문가 선택 |
## 과제
- ==로드 밸런싱==: 특정 전문가에 부하 집중 방지
- ==통신 비용==: 분산 환경에서 전문가 간 통신
- ==학습 불안정==: 게이팅 네트워크 학습 어려움
